{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filipino-English BPE Tokenizer Test\n",
    "\n",
    "This notebook tests the BPE tokenizer implementation for Filipino-English translation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies\n",
    "\n",
    "Run this cell if packages are not already installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment if you need to install packages\n",
    "# !pip install datasets tokenizers transformers torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c50ffffab5a74f61ac81109cfed70b54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/736 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cc1dba06dcd41b59ba5cf13e7255ba0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train_data.csv:   0%|          | 0.00/68.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0ed23943399454c8e26c239840d3e67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test_data.csv:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d75a16c0fc249db9daf69f6167f2cec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/84177 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfc38b9d8cd540849ceb35daccbddeb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/21057 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset structure:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['tagalog', 'english'],\n",
      "        num_rows: 84177\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['tagalog', 'english'],\n",
      "        num_rows: 21057\n",
      "    })\n",
      "})\n",
      "\n",
      "First training example:\n",
      "{'tagalog': ' Ilarawan kung ano ang makikita mo kung pupunta ka sa Grand Canyon.', 'english': 'Describe what you would see if you went to the Grand Canyon.'}\n"
     ]
    }
   ],
   "source": [
    "# Load the Filipino-English translation dataset\n",
    "ds = load_dataset(\"rhyliieee/tagalog-filipino-english-translation\")\n",
    "\n",
    "print(\"Dataset structure:\")\n",
    "print(ds)\n",
    "print(\"\\nFirst training example:\")\n",
    "print(ds[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Explore Dataset Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 84177\n",
      "Test samples: 21057\n",
      "Total samples: 105234\n",
      "\n",
      "=== Sample Translations ===\n",
      "\n",
      "Example 1:\n",
      "Tagalog:  Ilarawan kung ano ang makikita mo kung pupunta ka sa Grand Canyon.\n",
      "English: Describe what you would see if you went to the Grand Canyon.\n",
      "\n",
      "Example 2:\n",
      "Tagalog: Saang bansa ipinanganak si Pangulong Roosevelt?\n",
      "English: In what country was President Roosevelt born?\n",
      "\n",
      "Example 3:\n",
      "Tagalog:  Dahil sa pangalan ng kanta, hulaan ang genre ng kanta.\n",
      "English: Given a song name, predict the genre of the song.\n"
     ]
    }
   ],
   "source": [
    "# Check dataset sizes\n",
    "print(f\"Training samples: {len(ds['train'])}\")\n",
    "print(f\"Test samples: {len(ds['test'])}\")\n",
    "print(f\"Total samples: {len(ds['train']) + len(ds['test'])}\")\n",
    "\n",
    "# Sample a few examples\n",
    "print(\"\\n=== Sample Translations ===\")\n",
    "for i in range(3):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Tagalog: {ds['train'][i]['tagalog']}\")\n",
    "    print(f\"English: {ds['train'][i]['english']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train BPE Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokenizer on 210468 text samples...\n"
     ]
    }
   ],
   "source": [
    "# Combine train and test for tokenizer training\n",
    "# (Tokenizer needs to see all vocabulary, unlike model training)\n",
    "all_text = concatenate_datasets([ds[\"train\"], ds[\"test\"]])\n",
    "\n",
    "def text_generator():\n",
    "    \"\"\"Generator to yield all text samples from both languages\"\"\"\n",
    "    for row in all_text:\n",
    "        yield row[\"tagalog\"]\n",
    "        yield row[\"english\"]\n",
    "\n",
    "print(f\"Training tokenizer on {len(all_text) * 2} text samples...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training in progress...\n",
      "\n",
      "\n",
      "\n",
      "âœ“ Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Initialize tokenizer with BPE model\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "# Configure trainer\n",
    "trainer = BpeTrainer(\n",
    "    vocab_size=32000,\n",
    "    special_tokens=[\"[PAD]\", \"[UNK]\", \"[BOS]\", \"[EOS]\"]\n",
    ")\n",
    "\n",
    "# Train the tokenizer\n",
    "print(\"Training in progress...\")\n",
    "tokenizer.train_from_iterator(text_generator(), trainer=trainer)\n",
    "print(\"âœ“ Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer wrapped and ready to use!\n"
     ]
    }
   ],
   "source": [
    "# Wrap as a transformers-compatible tokenizer\n",
    "hf_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=tokenizer,\n",
    "    bos_token=\"[BOS]\",\n",
    "    eos_token=\"[EOS]\",\n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\"\n",
    ")\n",
    "\n",
    "print(\"Tokenizer wrapped and ready to use!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save and Reload Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Tokenizer saved to 'fil_en_bpe_tokenizer'\n",
      "âœ“ Tokenizer reloaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Save the tokenizer\n",
    "save_path = \"fil_en_bpe_tokenizer\"\n",
    "hf_tokenizer.save_pretrained(save_path)\n",
    "print(f\"âœ“ Tokenizer saved to '{save_path}'\")\n",
    "\n",
    "# Reload to verify it works\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(save_path)\n",
    "print(\"âœ“ Tokenizer reloaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Basic Tokenizer Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 32000\n",
      "Special tokens: ['[BOS]', '[EOS]', '[UNK]', '[PAD]']\n",
      "Special token IDs: [2, 3, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "# Test vocabulary size\n",
    "print(f\"Vocabulary size: {tokenizer.vocab_size}\")\n",
    "print(f\"Special tokens: {tokenizer.all_special_tokens}\")\n",
    "print(f\"Special token IDs: {tokenizer.all_special_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First 30 tokens in vocabulary:\n",
      "['Reduces', 'currency', 'napakagandang', 'makat', 'Div', 'particulate', 'å¤–', 'Contact', 'oss', 'iproseso', 'Bry', 'bumili', 'PDF', 'learns', 'igas', 'just', 'kad', 'pamamahagi', 'clo', 'katangiang', 'weekends', 'quantities', 'hospital', 'columns', 'siguradong', 'kilograms', 'churn', 'spin', 'magpapahusay', 'rooms']\n"
     ]
    }
   ],
   "source": [
    "# Inspect some tokens from vocabulary\n",
    "vocab = tokenizer.get_vocab()\n",
    "print(f\"\\nFirst 30 tokens in vocabulary:\")\n",
    "print(list(vocab.keys())[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test on Sample Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Tokenization Tests ===\n",
      "\n",
      "Original: Kumusta ka?\n",
      "Tokens: ['Kumusta', 'ka', '?']\n",
      "Token IDs: [14043, 665, 34]\n",
      "Token count: 3\n",
      "\n",
      "Original: Hello, how are you?\n",
      "Tokens: ['Hello', ',', 'how', 'are', 'you', '?']\n",
      "Token IDs: [5432, 15, 1527, 742, 839, 34]\n",
      "Token count: 6\n",
      "\n",
      "Original: Ang weather ngayon ay maganda.\n",
      "Tokens: ['Ang', 'weather', 'ngayon', 'ay', 'maganda', '.']\n",
      "Token IDs: [710, 3612, 3009, 649, 7444, 17]\n",
      "Token count: 6\n",
      "\n",
      "Original: Mahal kita.\n",
      "Tokens: ['Mahal', 'kita', '.']\n",
      "Token IDs: [11085, 2303, 17]\n",
      "Token count: 3\n",
      "\n",
      "Original: Good morning!\n",
      "Tokens: ['Good', 'morning', '!']\n",
      "Token IDs: [7155, 6821, 4]\n",
      "Token count: 3\n",
      "\n",
      "Original: Salamat sa iyong tulong.\n",
      "Tokens: ['Salamat', 'sa', 'iyong', 'tulong', '.']\n",
      "Token IDs: [5897, 641, 845, 3299, 17]\n",
      "Token count: 5\n"
     ]
    }
   ],
   "source": [
    "# Test sentences: Filipino, English, and code-mixed\n",
    "test_sentences = [\n",
    "    \"Kumusta ka?\",  # Filipino - How are you?\n",
    "    \"Hello, how are you?\",  # English\n",
    "    \"Ang weather ngayon ay maganda.\",  # Code-mixed - The weather today is beautiful\n",
    "    \"Mahal kita.\",  # Filipino - I love you\n",
    "    \"Good morning!\",  # English\n",
    "    \"Salamat sa iyong tulong.\",  # Filipino - Thank you for your help\n",
    "]\n",
    "\n",
    "print(\"=== Tokenization Tests ===\")\n",
    "for sent in test_sentences:\n",
    "    tokens = tokenizer.tokenize(sent)\n",
    "    token_ids = tokenizer.encode(sent, add_special_tokens=False)\n",
    "    print(f\"\\nOriginal: {sent}\")\n",
    "    print(f\"Tokens: {tokens}\")\n",
    "    print(f\"Token IDs: {token_ids}\")\n",
    "    print(f\"Token count: {len(tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Test Encoding and Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:  Ilarawan kung ano ang makikita mo kung pupunta ka sa Grand Canyon.\n",
      "\n",
      "Input IDs shape: torch.Size([1, 13])\n",
      "Input IDs: tensor([[ 2401,   854,  1011,   636,  5539,   723,   854, 17411,   665,   641,\n",
      "          9994, 12598,    17]])\n",
      "Attention mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "\n",
      "Decoded text: Ilarawan kung ano ang makikita mo kung pupunta ka sa Grand Canyon .\n"
     ]
    }
   ],
   "source": [
    "# Test full encode-decode cycle\n",
    "sample_text = ds[\"train\"][0][\"tagalog\"]\n",
    "\n",
    "print(f\"Original text: {sample_text}\")\n",
    "print()\n",
    "\n",
    "# Encode with special tokens\n",
    "encoded = tokenizer(sample_text, return_tensors=\"pt\")\n",
    "print(f\"Input IDs shape: {encoded['input_ids'].shape}\")\n",
    "print(f\"Input IDs: {encoded['input_ids']}\")\n",
    "print(f\"Attention mask: {encoded['attention_mask']}\")\n",
    "print()\n",
    "\n",
    "# Decode back to text\n",
    "decoded = tokenizer.decode(encoded['input_ids'][0])\n",
    "print(f\"Decoded text: {decoded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Batch Encoding Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch encoding results:\n",
      "Input IDs shape: torch.Size([3, 5])\n",
      "\n",
      "Input IDs:\n",
      "tensor([[ 9732, 11115,     4,     0,     0],\n",
      "        [14043,   665,  8781,  7467,    34],\n",
      "        [  710, 17996,   634,  1391,    17]])\n",
      "\n",
      "Attention mask:\n",
      "tensor([[1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1]])\n",
      "\n",
      "Decoded sequences:\n",
      "1. Mab uhay !\n",
      "2. Kumusta ka ngayong umaga ?\n",
      "3. Ang ganda ng panahon .\n"
     ]
    }
   ],
   "source": [
    "# Test batch encoding with padding\n",
    "batch_texts = [\n",
    "    \"Mabuhay!\",\n",
    "    \"Kumusta ka ngayong umaga?\",\n",
    "    \"Ang ganda ng panahon.\"\n",
    "]\n",
    "\n",
    "# Encode batch with padding\n",
    "batch_encoded = tokenizer(\n",
    "    batch_texts, \n",
    "    padding=True, \n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "print(\"Batch encoding results:\")\n",
    "print(f\"Input IDs shape: {batch_encoded['input_ids'].shape}\")\n",
    "print(f\"\\nInput IDs:\\n{batch_encoded['input_ids']}\")\n",
    "print(f\"\\nAttention mask:\\n{batch_encoded['attention_mask']}\")\n",
    "\n",
    "# Decode each sequence\n",
    "print(\"\\nDecoded sequences:\")\n",
    "for i, ids in enumerate(batch_encoded['input_ids']):\n",
    "    decoded = tokenizer.decode(ids, skip_special_tokens=True)\n",
    "    print(f\"{i+1}. {decoded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Calculate Tokenizer Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Tokenizer Statistics (100 samples) ===\n",
      "\n",
      "Tagalog:\n",
      "  Average tokens: 12.76\n",
      "  Min tokens: 5\n",
      "  Max tokens: 26\n",
      "\n",
      "English:\n",
      "  Average tokens: 11.06\n",
      "  Min tokens: 5\n",
      "  Max tokens: 28\n",
      "\n",
      "Average compression ratio: 1.15\n"
     ]
    }
   ],
   "source": [
    "# Calculate average tokens per sentence\n",
    "import numpy as np\n",
    "\n",
    "tagalog_lengths = []\n",
    "english_lengths = []\n",
    "\n",
    "# Sample 100 examples\n",
    "for i in range(min(100, len(ds['train']))):\n",
    "    tag_tokens = tokenizer.tokenize(ds['train'][i]['tagalog'])\n",
    "    eng_tokens = tokenizer.tokenize(ds['train'][i]['english'])\n",
    "    tagalog_lengths.append(len(tag_tokens))\n",
    "    english_lengths.append(len(eng_tokens))\n",
    "\n",
    "print(\"=== Tokenizer Statistics (100 samples) ===\")\n",
    "print(f\"\\nTagalog:\")\n",
    "print(f\"  Average tokens: {np.mean(tagalog_lengths):.2f}\")\n",
    "print(f\"  Min tokens: {np.min(tagalog_lengths)}\")\n",
    "print(f\"  Max tokens: {np.max(tagalog_lengths)}\")\n",
    "\n",
    "print(f\"\\nEnglish:\")\n",
    "print(f\"  Average tokens: {np.mean(english_lengths):.2f}\")\n",
    "print(f\"  Min tokens: {np.min(english_lengths)}\")\n",
    "print(f\"  Max tokens: {np.max(english_lengths)}\")\n",
    "\n",
    "print(f\"\\nAverage compression ratio: {np.mean(tagalog_lengths) / np.mean(english_lengths):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Test Unknown Token Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Unknown/Rare Token Handling ===\n",
      "\n",
      "Text: Supercalifragilisticexpialidocious\n",
      "Tokens: ['Super', 'cal', 'if', 'rag', 'ilis', 'tice', 'x', 'p', 'ial', 'ido', 'cious']\n",
      "Contains [UNK]: False\n",
      "\n",
      "Text: Pakikipagsapalaran\n",
      "Tokens: ['Pakikipagsapalaran']\n",
      "Contains [UNK]: False\n",
      "\n",
      "Text: ðŸŽ‰ðŸŽŠ\n",
      "Tokens: ['ðŸŽ‰', 'ðŸŽŠ']\n",
      "Contains [UNK]: False\n",
      "\n",
      "Text: test123\n",
      "Tokens: ['test', '123']\n",
      "Contains [UNK]: False\n"
     ]
    }
   ],
   "source": [
    "# Test with rare/unusual words\n",
    "unusual_texts = [\n",
    "    \"Supercalifragilisticexpialidocious\",  # Made-up English word\n",
    "    \"Pakikipagsapalaran\",  # Complex Filipino word\n",
    "    \"ðŸŽ‰ðŸŽŠ\",  # Emojis\n",
    "    \"test123\",  # Mixed alphanumeric\n",
    "]\n",
    "\n",
    "print(\"=== Unknown/Rare Token Handling ===\")\n",
    "for text in unusual_texts:\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    print(f\"\\nText: {text}\")\n",
    "    print(f\"Tokens: {tokens}\")\n",
    "    print(f\"Contains [UNK]: {'[UNK]' in tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Integration Test with Model Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Model Configuration Test ===\n",
      "Vocab size: 32000\n",
      "PAD token ID: 0\n",
      "BOS token ID: 2\n",
      "EOS token ID: 3\n",
      "UNK token ID: 1\n",
      "\n",
      "âœ“ All special tokens properly configured!\n"
     ]
    }
   ],
   "source": [
    "# Test tokenizer configuration for encoder-decoder model\n",
    "from transformers import EncoderDecoderConfig\n",
    "\n",
    "print(\"=== Model Configuration Test ===\")\n",
    "print(f\"Vocab size: {tokenizer.vocab_size}\")\n",
    "print(f\"PAD token ID: {tokenizer.pad_token_id}\")\n",
    "print(f\"BOS token ID: {tokenizer.bos_token_id}\")\n",
    "print(f\"EOS token ID: {tokenizer.eos_token_id}\")\n",
    "print(f\"UNK token ID: {tokenizer.unk_token_id}\")\n",
    "\n",
    "# Verify all special token IDs are set\n",
    "assert tokenizer.pad_token_id is not None, \"PAD token ID not set!\"\n",
    "assert tokenizer.bos_token_id is not None, \"BOS token ID not set!\"\n",
    "assert tokenizer.eos_token_id is not None, \"EOS token ID not set!\"\n",
    "assert tokenizer.unk_token_id is not None, \"UNK token ID not set!\"\n",
    "\n",
    "print(\"\\nâœ“ All special tokens properly configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "- Loading the Filipino-English dataset\n",
    "- Training a BPE tokenizer on both languages\n",
    "- Saving and reloading the tokenizer\n",
    "- Testing tokenization on various inputs\n",
    "- Batch encoding with padding\n",
    "- Computing tokenizer statistics\n",
    "- Verifying model integration readiness\n",
    "\n",
    "The tokenizer is now ready to be used with an encoder-decoder translation model!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
